{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7edb24",
   "metadata": {},
   "source": [
    "# Eye Open/Closed Detection Model Training for OACE Dataset\n",
    "\n",
    "The **MRL Eye Dataset** is a well-curated and high-quality dataset captured using **infrared (IR) cameras**. It contains extensive augmentation and is highly useful for model training under controlled IR imaging conditions.  \n",
    "\n",
    "However, since the MRL dataset consists of **IR eye images**, models trained solely on it may not perform optimally when applied to **visible-light (RGB) webcam images**. This is primarily due to the **domain gap** between IR and visible-light imagery — differences in illumination, color, texture, and reflection characteristics can significantly impact model generalization.\n",
    "\n",
    "Our target application involves **real-time eye open/closed state classification using a standard webcam**, which operates in the visible spectrum. Therefore, to achieve higher accuracy and robustness under real-world lighting conditions, we decided to train our model using a **visible-light eye image dataset**.\n",
    "\n",
    "For this purpose, we utilized the **OACE Dataset**, which contains labeled **open-eye** and **closed-eye** samples captured under standard visible-light conditions. This dataset better represents the operational environment of a typical webcam-based system.\n",
    "\n",
    "In this notebook, we:\n",
    "- Explore and preprocess the **OACE dataset**.  \n",
    "- Train a deep learning model to classify **eye state (open/closed)** in visible-light images.  \n",
    "- Compare its performance with models trained on IR (MRL) data.  \n",
    "- Evaluate the model’s suitability for **real-time eye state detection** in webcam applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8122c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (edit these to match your dataset)\n",
    "data_dir = Path('data/OACE_Eye_Dataset')\n",
    "train_dir = data_dir / 'train'\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),#input size for mobilenetv2\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "#Data loaders\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "test_ds = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "batch_size = 32 #according to system capability\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print('Classes:', train_ds.classes)\n",
    "class_to_idx = train_ds.class_to_idx\n",
    "print('Class to idx mapping:', class_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cd9fc",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f47a6",
   "metadata": {},
   "source": [
    "**Preprocessing function**\n",
    "\n",
    "The data in OACE folder was preprocessed and replaced using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_eye_image(\n",
    "    img_path: str,\n",
    "    output_size=(82, 82),\n",
    "    gamma_value=0.6,\n",
    "    clip_limit=2.0,\n",
    "    tile_size=(6, 6),\n",
    "    noise_std=6,\n",
    "    brightness_factor=1.1,\n",
    "    dark_boost_strength=0.5,\n",
    "    target_mean=83,#from mrl dataset\n",
    "    target_std=15.5,#from mrl dataset\n",
    "):\n",
    "\n",
    "    #load image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"❌ Cannot load image: {img_path}\")\n",
    "\n",
    "    #grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, 5, 30, 30)  #preserves edges\n",
    "    gray = cv2.equalizeHist(gray)  #global normalization\n",
    "\n",
    "    #CLAHE for local contrast normalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "    balanced = clahe.apply(gray)\n",
    "\n",
    "    #Gamma correction (compress highlights)\n",
    "    invGamma = 1.0 / gamma_value\n",
    "    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(256)]).astype(\"uint8\")\n",
    "    gamma_corrected = cv2.LUT(balanced, table)\n",
    "\n",
    "    #soft smoothing (to remove harsh local contrast)\n",
    "    smoothed = cv2.bilateralFilter(gamma_corrected, 3, 40, 40)\n",
    "\n",
    "    #add subtle Gaussian noise\n",
    "    noise = np.random.normal(0, noise_std, smoothed.shape)\n",
    "    noisy = np.clip(smoothed.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #brighten darker regions selectively\n",
    "    img_f = noisy.astype(np.float32)\n",
    "    boost = dark_boost_strength * (1 - img_f / 255.0) * 70\n",
    "    brightened = np.clip(img_f + boost, 0, 255)\n",
    "\n",
    "    #global brightness boost\n",
    "    brightened = np.clip(brightened * brightness_factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #normalize histogram to MRL mean/std\n",
    "    mean, std = brightened.mean(), brightened.std()\n",
    "    normalized = np.clip((brightened - mean) / (std + 1e-6) * target_std + target_mean, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #resize to match MRL format\n",
    "    final_resized = cv2.resize(normalized, output_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return final_resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720571b",
   "metadata": {},
   "source": [
    "**use of the function defined above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_and_replace(base_dir=\"OACE/test\", dry_run=False, recurse=True, output_size=(82,82), gamma_value=0.6)\n",
    "#preprocess_and_replace(base_dir=\"OACE/train\", dry_run=False, recurse=True, output_size=(82,82), gamma_value=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071d2ed",
   "metadata": {},
   "source": [
    "## Model (Transfer Learning: MobileNetV2 backbone)\n",
    "Using a pretrained backbone speeds up convergence and often performs better than training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing mobilenetv2\n",
    "#model = models.mobilenet_v2(pretrained=True) \n",
    "model = models.mobilenet_v2(weights=None)\n",
    "#replacing classifier\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(num_features, 2)\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38423387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and validation functions\n",
    "from copy import deepcopy\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "#training loop\n",
    "best_model_wts = deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - train_loss: {train_loss:.5f} acc: {train_acc:.5f} | val_loss: {val_loss:.5f} val_acc: {val_acc:.5f}')\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = deepcopy(model.state_dict())\n",
    "#using best model weights according to validation accuracy\n",
    "model.load_state_dict(best_model_wts)\n",
    "print('Best val acc:', best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "model_path = 'models/eye_detector_mobilenetv2_(OACE_dataset).pth'\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'class_to_idx': class_to_idx}, model_path)\n",
    "print('Saved model to', model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
