{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318b82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe221042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Class labels: {0: 'close', 1: 'open'}\n"
     ]
    }
   ],
   "source": [
    "#model loading\n",
    "model_path = 'models/eye_detector_mobilenetv2_(OACE_dataset).pth'\n",
    "\n",
    "#recreated the same architecture\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(num_features, 2)   # 2 classes: open / closed\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#load checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "#restore weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#restore label mapping\n",
    "idx_to_class = {v: k for k, v in checkpoint['class_to_idx'].items()}\n",
    "\n",
    "#move model to device and set to eval mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "#image transformer\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(\"Class labels:\", idx_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54f335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_pil, model, transform, device, idx_to_class):\n",
    "    model.eval()\n",
    "    x = transform(img_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        prob = torch.softmax(out, dim=1)\n",
    "        pred = prob.argmax(dim=1).item()\n",
    "        conf = prob.max().item()\n",
    "    return idx_to_class[pred], conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a247cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eye_image(\n",
    "    img,#eye image\n",
    "    output_size=(82, 82),#final output size\n",
    "    gamma_value=0.6,#gamma correction value\n",
    "    clip_limit=2.0,#CLAHE clip limit\n",
    "    tile_size=(6, 6),#tile size for CLAHE\n",
    "    noise_std=6,#noise standard deviation\n",
    "    brightness_factor=1.1,#adjust overall brightness\n",
    "    dark_boost_strength=0.5,#adjust boost for dark regions\n",
    "    target_mean=83,#global target mean mrl\n",
    "    target_std=15.5,#global target standard deviation mrl\n",
    "):\n",
    "    #loadd image\n",
    "    if img is None:\n",
    "        raise ValueError(f\"cannot load image: {img_path}\")\n",
    "\n",
    "    #grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, 5, 30, 30)  #preserves edges\n",
    "    gray = cv2.equalizeHist(gray)  #global normalization\n",
    "\n",
    "    #CLAHE for local contrast normalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "    balanced = clahe.apply(gray)\n",
    "\n",
    "    #gamma correction (to compress highlights)\n",
    "    invGamma = 1.0 / gamma_value\n",
    "    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(256)]).astype(\"uint8\")\n",
    "    gamma_corrected = cv2.LUT(balanced, table)\n",
    "\n",
    "    #soft smoothing (to remove harsh local contrast)\n",
    "    smoothed = cv2.bilateralFilter(gamma_corrected, 3, 40, 40)\n",
    "\n",
    "    #add subtle Gaussian noise\n",
    "    noise = np.random.normal(0, noise_std, smoothed.shape)\n",
    "    noisy = np.clip(smoothed.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #brighten darker regions selectively\n",
    "    img_f = noisy.astype(np.float32)\n",
    "    boost = dark_boost_strength * (1 - img_f / 255.0) * 70\n",
    "    brightened = np.clip(img_f + boost, 0, 255)\n",
    "\n",
    "    #global brightness boost\n",
    "    brightened = np.clip(brightened * brightness_factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #normalize histogram to MRL mean/std\n",
    "    mean, std = brightened.mean(), brightened.std()\n",
    "    normalized = np.clip((brightened - mean) / (std + 1e-6) * target_std + target_mean, 0, 255).astype(np.uint8)\n",
    "\n",
    "    #resize\n",
    "    final_resized = cv2.resize(normalized, output_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return final_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64156412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "SCALE = 4  # how large the square is relative to iris diameter\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=5,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "#iris landmarks\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "def extract_eyes_from_frame(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        return []\n",
    "\n",
    "    eyes = []\n",
    "\n",
    "    for face_id, face_landmarks in enumerate(results.multi_face_landmarks, start=1):\n",
    "        landmarks = face_landmarks.landmark\n",
    "\n",
    "        def get_iris_center_and_radius(iris_indices):\n",
    "            pts = np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in iris_indices])\n",
    "            (cx, cy), radius = cv2.minEnclosingCircle(pts)\n",
    "            return int(cx), int(cy), int(radius)\n",
    "\n",
    "        #left eye\n",
    "        lx, ly, lr = get_iris_center_and_radius(LEFT_IRIS)\n",
    "        left_size = int(lr * SCALE)\n",
    "        lx1, ly1 = max(lx - left_size, 0), max(ly - left_size, 0)\n",
    "        lx2, ly2 = min(lx + left_size, w), min(ly + left_size, h)\n",
    "        left_eye_crop = frame[ly1:ly2, lx1:lx2]\n",
    "\n",
    "        #right eye\n",
    "        rx, ry, rr = get_iris_center_and_radius(RIGHT_IRIS)\n",
    "        right_size = int(rr * SCALE)\n",
    "        rx1, ry1 = max(rx - right_size, 0), max(ry - right_size, 0)\n",
    "        rx2, ry2 = min(rx + right_size, w), min(ry + right_size, h)\n",
    "        right_eye_crop = frame[ry1:ry2, rx1:rx2]\n",
    "\n",
    "        eyes.append((left_eye_crop, right_eye_crop, face_id,(lx1, ly1, lx2, ly2, rx1, ry1, rx2, ry2)))\n",
    "\n",
    "    return eyes\n",
    "\n",
    "\n",
    "#real time eye prediciton\n",
    "cap = cv2.VideoCapture(1)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    eyes_data = extract_eyes_from_frame(frame)\n",
    "\n",
    "    for left_eye, right_eye, face_id, (lx1, ly1, lx2, ly2, rx1, ry1, rx2, ry2) in eyes_data:\n",
    "        preds = []\n",
    "\n",
    "        for eye_img in [left_eye, right_eye]:\n",
    "            processed = preprocess_eye_image(eye_img)\n",
    "            if processed is None:\n",
    "                preds.append(\"unknown\")\n",
    "                continue\n",
    "            #pred_label, conf = predict_image(Image.fromarray(processed).convert('RGB'), model, transformer, DEVICE, idx_to_class)\n",
    "            pred_label = predict_image(Image.fromarray(processed).convert('RGB'), model, transformer, DEVICE, idx_to_class)[0]\n",
    "            preds.append(pred_label)\n",
    "\n",
    "        left_state, right_state = preds\n",
    "\n",
    "        #draw colored boxes based on eye state\n",
    "        left_color = (0, 255, 0) if left_state.lower() == \"open\" else (0, 0, 255)\n",
    "        right_color = (0, 255, 0) if right_state.lower() == \"open\" else (0, 0, 255)\n",
    "\n",
    "        cv2.rectangle(frame, (lx1, ly1), (lx2, ly2), left_color, 2)\n",
    "        cv2.rectangle(frame, (rx1, ry1), (rx2, ry2), right_color, 2)\n",
    "\n",
    "        #label predictions\n",
    "        cv2.putText(frame, f\"P{face_id} L:{left_state} R:{right_state}\",\n",
    "                    (lx1, ly1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-time Eye State Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
